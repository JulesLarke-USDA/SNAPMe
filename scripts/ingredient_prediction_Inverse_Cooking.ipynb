{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9508ded4",
   "metadata": {},
   "source": [
    "### Title: ingredient_prediction_Inverse_Cooking\n",
    "\n",
    "__Date:__ 5/8/23\n",
    "\n",
    "__Author:__ Jules Larke\n",
    " \n",
    "__Purpose__  \n",
    "Generate F1 scores for per meal image based on text descriptions predicted with the Inverse Cooking algorithm\n",
    "\n",
    "__Required Input Files__\n",
    "\n",
    "  - **master_SNAPME_linkfile.csv** - master file linking images with participant metadata (downloaded from Ag Data Commons)\n",
    "  - **snapme_allimages.out** - Output from FB Inverse Cooking algorithm\n",
    "  - **fndds_2018.csv** - Food and Nutrition Database for Dietary Studies 2017-2018 - used for ingredientization\n",
    "  - **snapme_NA_replaced_inplace_100722.csv** - replaced discontinued foodcodes from previous version of FNDDS used to record diets via ASA24\n",
    "  - **snapme_single_ingred_parsed_r1m.csv** - foods parsed into their ingredient from the R1M database  \n",
    "  - **snapme_ingredientized_manual_fixes__tokens_cleaned_012623.csv** - Foods from ASA24 that could not be ingredientized, primarily baked goods. These items were matched to Branded Foods to get ingredient lists.\n",
    "\n",
    "__Output__\n",
    "- **snapme_inv_cook_f1_score_050823.csv** Will be the input for data visualization and statistics in f1_score_testing_visualization.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b9fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "#nltk.data.path.append('/Users/jules.larke/opt/anaconda3/)\n",
    "import string\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ed1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data\n",
    "documents = pd.read_csv('../input/master_SNAPME_linkfile.csv')\n",
    "labels = pd.read_csv('../input/snapme_allimages.out', sep=\":\", header=None)\n",
    "fndds = pd.read_csv('../input/fndds2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1120dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed data to replace original data with discontinued foodcodes or ingredientized data R1M\n",
    "snapme_to_merge = pd.read_csv('../input/snapme_NA_replaced_inplace_100722.csv') # walked the foodcode snapme nas forward to FNDDS17-18 to replace those discontinued foodcodes\n",
    "r1m = pd.read_csv('../input/snapme_single_ingred_parsed_r1m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42fded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foods from ASA24 that could not be ingredientized, primarily baked goods. These items were matched to Branded Foods to get ingredient lists.\n",
    "baked = pd.read_csv('../input/snapme_ingredientized_manual_fixes__tokens_cleaned_012623.csv')\n",
    "baked = baked[baked['manual_fix'] == 'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "405cb0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.rename(columns={0:'filename',1:'inverse_cook'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3e3f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.filename = labels.filename.str.rstrip() # remove single whitespace at end of each string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b0d7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.inverse_cook = labels.inverse_cook.str.lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00bd1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "fndds_multi = fndds.loc[fndds['Seq num'] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a02a466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fndds_multi = fndds[fndds['Food code'].isin(fndds_multi['Food code'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a23ce298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/kcjp2s310sx44jpn8yqs8y9c929k6r/T/ipykernel_37044/2426629251.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fndds_multi[\"single_multi\"] = 'multi'\n"
     ]
    }
   ],
   "source": [
    "fndds_multi[\"single_multi\"] = 'multi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8314266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fndds_single = fndds[~fndds['Food code'].isin(fndds_multi['Food code'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c65cb166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/kcjp2s310sx44jpn8yqs8y9c929k6r/T/ipykernel_37044/2610464955.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fndds_single['single_multi'] = 'single'\n"
     ]
    }
   ],
   "source": [
    "fndds_single['single_multi'] = 'single'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e9a0cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fndds_sm = pd.concat([fndds_single, fndds_multi], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21a580d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fndds_sm.rename(columns={'Main food description':'Food_Description', 'Food code': 'FoodCode'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cb86214",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_one_row_filename = documents.groupby(\"filename\").filter(lambda x: len(x) == 1) #rename to something differnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c65434f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_single_no_na = snap_one_row_filename[~snap_one_row_filename['filename'].isin(snapme_to_merge['filename'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff6e17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_single_no_na = snap_single_no_na.loc[:,'subject_id':'Food_Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1f22b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_single = pd.concat([snap_single_no_na, snapme_to_merge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "692a0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = fndds_sm[['FoodCode', 'single_multi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c87a4dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = tmp.drop_duplicates(subset='FoodCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "399112fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_tmp = pd.merge(snap_single, tmp1, on='FoodCode', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44555e65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r1m.drop(columns=['Food_Description', 'clean_punct', 'clean_text', 'title', 'ingredients'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76e0fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1m.rename(columns={'parsed':'Food_Description'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0c6b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_merge = pd.concat([snap_tmp, r1m], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66b353a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = documents[~documents['filename'].isin(snap_merge.filename)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1d0268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = doc2.loc[:, :'Food_Description'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ac3e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_all = pd.concat([snap_merge, doc2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53c2c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all food_descriptions for a given image for ASA24 data:\n",
    "docs = snap_all.groupby('filename')['Food_Description'].apply(', '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87063e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.merge(docs, labels, on='filename', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0aadaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = string.punctuation[0:11] + string.punctuation[13:] # remove '-' from the list of punctuation. This is needed for the tokenizer in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c39034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['ns', 'nfs', '', 'dry', 'water', 'black', 'brown', 'bayo', 'cut', 'cooked', 'cooking', 'as', 'to', 'of', 'flavor', 'fat', 'eaten', 'made', 'with', 'raw', 'brewed', 'fried', 'eaten', 'toasted', 'boiled', 'from', 'fresh', 'type', 'coated', 'baked', 'or', 'broiled', 'part', 'and', 'method', 'skin', 'not', 'stewed', 'canned', 'table', 'reduced', 'fat', 'added', 'in', 'hot', 'granulated', 'ground', 'Mexican', 'blend', 'flavored',  'home', 'recipe', 'purchased', 'c', 'at', 'a', 'bakery', 'replacement', 'free', 'powder', '0', '1', '2', '20', '99', '100']\n",
    "stopwords = stopwords # + drop # added words from list of the intersection of unique words \n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in punct])\n",
    "    tokens = re.split('[-\\W+]', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    text = [wn.lemmatize(word) for word in tokens if word not in stopwords]\n",
    "    return set(text)\n",
    "\n",
    "df_all['clean_description'] = df_all['Food_Description'].apply(lambda x: clean_text(x.lower()))\n",
    "df_all['clean_inv_cook'] = df_all['inverse_cook'].apply(lambda x: clean_text(x.lower()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82f9bdf",
   "metadata": {},
   "source": [
    "#### determine stopwords to remove based on tokens that do not specify a food/ingredient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80b1366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapme_list = [item for sublist in df_all['clean_description'] for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ad1faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapme_unique = list(set(snapme_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddf8422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_list = [item for sublist in df_all['clean_inv_cook'] for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb41c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_unique = list(set(inv_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb3f3f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect = list(set(snapme_list) & set(inv_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aebf3bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "baked_keep = ['garbanzo', 'turmeric', 'tapioca', 'mango', 'caraway', 'mulberry', 'poppy', 'goji', 'currant',\n",
    "             'barley', 'cottonseed', 'carob', 'chicory', 'cacao', 'semolina', 'jalapeno', 'cherry', 'zucchini']\n",
    "# list of tokens to keep from the branded foods list of ingredients\n",
    "# these tokens consisty only of words that specifically describe a food item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57c5b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapme_only = set(snapme_unique) - set(intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb445974",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = ['applesauce', 'agave', 'alfalfa', 'artichoke', 'bagel', 'beer', 'beet',\n",
    "        'batter', 'berry', 'biryani', 'biscuit', 'boysenberry', 'brownie', 'bratwurst', 'buckwheat',\n",
    "        'bulgur', 'burger', 'burrito', 'catfish', 'cake', 'catsup', 'cayenne', 'chex', 'consomme',\n",
    "        'cheddar', 'cheerio', 'cheeseburger', 'cheetos', 'chia', 'chickpea', 'chive', 'crab', 'cottage',\n",
    "        'cider', 'clam', 'crepe', 'croissant', 'cod', 'coleslaw', 'collard', 'chowder', 'cashew', 'coriander',\n",
    "        'cornbread', 'cornmeal', 'cornstarch', 'croquette', 'crouton', 'cupcake', 'crunchberries', 'cumin',\n",
    "        'custard', 'dough', 'date', 'doughnut', 'dough', 'dill', 'doritos', 'drumstick', 'enchilada',\n",
    "        'espresso', 'falafel', 'flatbread', 'flauta', 'frankfurter', 'frosting', 'fudge', 'fritter',\n",
    "        'flaxseed', 'flax', 'focaccia', 'gingersnap', 'gnocchi', 'groat', 'guacamole', 'guava',\n",
    "        'goldfish', 'gouda', 'graham', 'granola', 'grapefruit', 'gravy', 'grit', 'huckleberry',\n",
    "        'hamburger', 'icing','hazelnut', 'horchata', 'hummus', 'jello', 'jelly', 'jicama',\n",
    "        'jambalaya', 'kimchi', 'lard', 'lamb', 'latte', 'leek', 'licorice', 'lobster', 'margarine',\n",
    "        'macadamia', 'marmalade', 'minestrone', 'meatball', 'meringue', 'naan', 'nacho', 'nectarine',\n",
    "        'mussel', 'meat', 'muffin', 'mozzarella', 'muesli', 'muenster', 'omelet', 'oatmeal', 'okra',\n",
    "        'omelet', 'oyster', 'oolong', 'oregano', 'pancake', 'parmesan', 'parsnip', 'pastry', 'persimmon',\n",
    "        'pesto', 'paprika', 'pie', 'pine', 'pinto', 'pistachio', 'pita', 'pistashio', 'pizza', 'plantain',\n",
    "        'plum', 'pretzel', 'provolone', 'pudding',   'pomegranate',  'pickle', 'pastry', 'quesadilla',\n",
    "        'quiche', 'rapeseed', 'ranch', 'ravioli', 'ravioli', 'ricotta', 'roquefort', 'ritz', 'romaine',\n",
    "        'roquefort', 'rose', 'rye', 'safflower', 'sage', 'salad', 'salami', 'sandwich', 'sauerkraut',\n",
    "        'scone', 'seaweed', 'sesame', 'shallot', 'seafood', 'serrano', 'shortening', 'smoothie', 'snowpea',\n",
    "        'snowpeas', 'soda', 'soy', 'sparerib', 'stevia', 'soybean', 'soup', 'stroganoff', 'sushi', 'stuffing',\n",
    "        'sparerib', 'swiss', 'stew', 'swordfish', 'taco', 'tamale', 'tamari', 'taquito', 'tostada', 'thyme',\n",
    "        'tuna', 'turkey', 'tzatziki', 'vegetable', 'triscuit', 'tangerine', 'teriyaki', 'turnip', 'tilapia',\n",
    "        'tiramisu', 'tortellini', 'turnover', 'whopper', 'waffle', 'walnut', 'wheat', 'whiskey', 'wonton',\n",
    "        'worcestershire', 'yolk'] + baked_keep\n",
    "# words that are distinct food items to keep \n",
    "## using this list as stopwords lowers average F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "645f2560",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = list(set(snapme_only) - set(keep))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d6352",
   "metadata": {},
   "source": [
    "#### apply text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ac63da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['ns', 'nfs', '', 'water', 'dry', 'black', 'brown', 'bayo', 'cut', 'cooked', 'cooking', 'as', 'to', 'of', 'flavor', 'fat', 'eaten', 'made', 'with', 'raw', 'brewed', 'fried', 'eaten', 'toasted', 'boiled', 'from', 'fresh', 'type', 'coated', 'baked', 'or', 'broiled', 'part', 'and', 'method', 'skin', 'not', 'stewed', 'canned', 'table', 'reduced', 'fat', 'added', 'in', 'hot', 'granulated', 'ground', 'Mexican', 'blend', 'flavored',  'home', 'recipe', 'purchased', 'c', 'at', 'a', 'bakery', 'replacement', 'free', 'powder', '0', '1', '2', '20', '99', '100']\n",
    "stopwords = stopwords + drop # added words from list of the intersection of unique words \n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in punct])\n",
    "    tokens = re.split('[-\\W+]', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    text = [wn.lemmatize(word) for word in tokens if word not in stopwords]\n",
    "    return set(text)\n",
    "\n",
    "df_all['clean_description'] = df_all['Food_Description'].apply(lambda x: clean_text(x.lower()))\n",
    "df_all['clean_inv_cook'] = df_all['inverse_cook'].apply(lambda x: clean_text(x.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0114c2a0",
   "metadata": {},
   "source": [
    "#### F1 scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6b0722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_detection(a,b):\n",
    "    tp = sum(any(m == L for m in a) for L in b)\n",
    "    fn = len(a) - tp\n",
    "    fp = len(b) - sum(any(m == L for m in b) for L in a)\n",
    "    try:\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = (2 * precision * recall)/(precision + recall)\n",
    "        return(f1)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "883f72da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.set_index('filename', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "247de38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "baked.set_index('filename', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08a0e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "baked['Full_Ingredient_List_clean'] = baked['Full_Ingredient_List'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7fc84e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['clean_description'].update(baked.Full_Ingredient_List_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "301ed550",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce4170b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [string_detection(x, y) for x, y in zip(df_all['clean_description'], df_all['clean_inv_cook'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25021d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['F1_score'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1af0302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22989004263257837"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.F1_score.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5fba022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "baked.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a88c89",
   "metadata": {},
   "source": [
    "#### create metadata for which database ingredients were parsed from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0b2c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['ingred_parsed_from'] = 'FNDDS1718'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb6512a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.loc[df_all.filename.isin(r1m.filename), ['ingred_parsed_from']] = 'R1M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c850a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.loc[df_all.filename.isin(baked.filename), ['ingred_parsed_from']] = 'BFDB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d1c62ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_meta = documents[['filename', 'FoodCode', 'Occ_Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3864ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_1 = pd.merge(df_all, doc_meta, how='inner', on='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "31ce6b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_1['FoodCode'] = tmp_1['FoodCode'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08d7bb",
   "metadata": {},
   "source": [
    "#### metadata for counting number of foodcodes, images..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5cdabed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp_1['food_count'] = tmp_1[['filename','FoodCode']].groupby(['filename'])['FoodCode'].transform(lambda x: x[x.str.contains('0|1|2|3|4|5|6|7|8|9')].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b29520d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_1['image_count'] = tmp_1[['filename','FoodCode']].groupby(['filename'])['FoodCode'].transform(lambda x: x[x.str.contains('0|1|2|3|4|5|6|7|8|9')].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b683e27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp_1['multi_codes'] = tmp_1[['filename','FoodCode']].groupby(['filename'])['FoodCode'].transform(lambda x: ', '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a8cf455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp_1 = tmp_1[['filename','multi_codes', 'food_count', 'Occ_Name']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf7d3de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1463, 10)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snap_pred_f1 = pd.merge(df_all, tmp_1, on='filename', how='left').drop_duplicates(subset='filename')\n",
    "snap_pred_f1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e39caa",
   "metadata": {},
   "source": [
    "#### recode eating occasion and bin foodcodes to 1, 2-3 or 4+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f4e11f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def occ_groups(series):\n",
    "    if series == 1:\n",
    "        return \"Breakfast\"\n",
    "    elif series == 2:\n",
    "        return \"Brunch\"\n",
    "    elif series == 3:\n",
    "        return \"Lunch\"\n",
    "    elif series == 4:\n",
    "        return \"Dinner\"\n",
    "    elif series == 6:\n",
    "        return \"Snack\"\n",
    "    elif series == 7:\n",
    "        return \"Drink\"\n",
    "    elif series == 8:\n",
    "        return \"Supplement\"\n",
    "\n",
    "snap_pred_f1['Occ_Name'] = snap_pred_f1['Occ_Name'].apply(occ_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "86cf5121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def food_count_groups(series):\n",
    "    if series in range(1,2):\n",
    "        return \"1\"\n",
    "    elif series in range(2,4):\n",
    "        return \"2-3\"\n",
    "    elif series in range(4,18):\n",
    "        return \"4+\"\n",
    "\n",
    "snap_pred_f1['food_count_groups'] = snap_pred_f1['food_count'].apply(food_count_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3271a",
   "metadata": {},
   "source": [
    "#### get counts for metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8fd16398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Occ_Name \n",
       "Breakfast    496\n",
       "Dinner       342\n",
       "Lunch        300\n",
       "Snack        231\n",
       "Drink         81\n",
       "Brunch         5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snap_pred_f1[['Occ_Name']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b35a7df9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "food_count_groups\n",
       "1                    551\n",
       "2-3                  467\n",
       "4+                   445\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snap_pred_f1[['food_count_groups']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "553d4354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "snap_pred_f1['coffee_tea_count'] = snap_pred_f1.groupby(['filename', 'Occ_Name'])['Food_Description'].transform(lambda x: x[x.str.contains('Coffee|Tea', case=False)].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "61860d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Occ_Name   coffee_tea_count\n",
       "Brunch     0.0                   5\n",
       "Lunch      1.0                  12\n",
       "Snack      1.0                  12\n",
       "Drink      0.0                  14\n",
       "Dinner     1.0                  18\n",
       "Drink      1.0                  67\n",
       "Breakfast  1.0                 114\n",
       "Snack      0.0                 219\n",
       "Lunch      0.0                 288\n",
       "Dinner     0.0                 324\n",
       "Breakfast  0.0                 382\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snap_pred_f1[['Occ_Name', 'coffee_tea_count']].value_counts(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd70844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_pred_f1.to_csv('../otuput/snapme_inv_cook_f1_score_050823.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
